{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vou explorar vários métodos de chunking que é um passo essencial para a utilização do método RAG. Esse notebook foi inspirado no repositório (https://github.com/FullStackRetrieval-com/RetrievalTutorials/tree/main) do GitHub\n",
    "### Métodos de chunking:\n",
    "\n",
    "- Level 1: Character Splitting - Simple static character chunks of data\n",
    "- Level 2: Recursive Character Text Splitting - Recursive chunking based on a list of separators\n",
    "- Level 3: Document Specific Splitting - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "- Level 4: Semantic Splitting - Embedding walk based chunking\n",
    "- Level 5: Agentic Splitting - Experimental method of splitting text with an agent-like system. Good for if you believe that token cost will trend to $0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: Semantic Splitting: Embedding based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gerar frases pequenas isoladas.\n",
    "- Gerar sentenças compostas sequencias\n",
    "- Gerar embeddings para cada sentença composta sequencial\n",
    "- Calcular as similaridades sequenciais para cada par de embeddings sequencias das sentenças compostas sequenciais\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be5b9fc16134994b79cd450c4fc53f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alber\\.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e3f7531e4145be8a82fab4a4861576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c97a9f80bc4e71b1fe135b09d216cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c92d3986ea84ab8b671fd02bfb15016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca614b2dcec346e5882ffbd6e39ff333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa2d6929d404db3af127f02f4c926ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.8618e-02, -7.4881e-02,  6.1885e-01,  1.9511e-02,  4.1850e-01,\n",
      "         -2.4749e-01,  1.1073e-01, -1.4529e-01,  4.0987e-01, -1.2605e-01,\n",
      "          4.0625e-01,  4.0208e-01,  8.9136e-02, -1.4250e-01,  3.3911e-02,\n",
      "         -3.1162e-02,  4.6844e-01,  1.9039e-01,  1.0885e-02, -2.9410e-01,\n",
      "         -5.2575e-01,  9.2692e-02,  2.6181e-01, -2.8897e-01,  2.1638e-01,\n",
      "          1.0893e-01, -2.8880e-01, -2.2818e-01,  3.4104e-01, -1.8739e-01,\n",
      "         -8.7211e-02,  2.4347e-01, -3.5235e-01,  1.7793e-01,  4.5271e-01,\n",
      "          2.2540e-01,  1.7696e-01,  1.0532e-02,  3.8700e-02,  5.2939e-02,\n",
      "          5.5013e-02, -1.7833e-01, -2.5059e-01,  4.1585e-02,  2.1694e-01,\n",
      "         -4.0593e-01,  2.7265e-02, -3.2302e-02, -6.0373e-01,  3.9893e-02,\n",
      "          5.4952e-01, -6.5261e-02, -5.1407e-01,  3.5545e-01,  3.1279e-01,\n",
      "         -3.0938e-01, -2.7651e-02,  1.4289e-01, -1.0875e-01, -1.0769e-01,\n",
      "         -5.3027e-02,  2.8557e-01,  4.3023e-01,  1.4881e-01, -2.2273e-01,\n",
      "          4.3958e-01,  4.5265e-01, -1.8614e-01,  4.1948e-02,  5.9291e-01,\n",
      "         -3.3597e-01,  2.6588e-01, -1.1415e-01,  6.6723e-02, -3.0502e-01,\n",
      "          3.3455e-01, -2.2241e-01,  6.8263e-02, -1.7411e-01, -1.5947e-01,\n",
      "          1.3553e-01,  6.5626e-02, -2.1470e-01,  3.4769e-01, -2.1467e-01,\n",
      "          4.1087e-01,  3.6058e-01, -1.0201e-01,  2.7729e-02, -8.3166e-02,\n",
      "          5.7785e-01, -3.0189e-01,  2.0826e-01,  3.9521e-01, -7.9358e-02,\n",
      "          5.2109e-01,  2.2445e-01, -3.6063e-02, -1.7472e-01,  1.1366e-01,\n",
      "         -4.0456e-02,  3.2581e-02,  5.4152e-01, -2.5530e-01,  4.9364e-01,\n",
      "         -4.3700e-02, -1.0970e-01, -1.2362e-02, -1.8644e-01,  5.4992e-01,\n",
      "          1.4277e-01,  9.9149e-02,  1.2300e-01,  1.4634e-01,  6.8903e-01,\n",
      "         -4.1208e-01, -1.0623e-01, -3.5394e-01,  2.6563e-01, -3.1321e-01,\n",
      "         -3.7110e-02,  2.6620e-01,  2.6999e-01,  5.0580e-02, -1.1336e-01,\n",
      "         -1.6624e-01,  8.1107e-03, -9.5331e-02,  3.0924e-02, -1.7000e-01,\n",
      "          3.4304e-01, -2.1787e-01, -3.1075e-01, -1.7829e-01, -8.8770e-02,\n",
      "         -5.7430e-02,  2.9179e-02,  1.5123e-01,  3.5609e-01, -6.8522e-02,\n",
      "          5.0692e-02, -2.8048e-01, -2.6548e-01,  1.8553e-01, -2.3793e-01,\n",
      "         -5.1836e-02, -3.8022e-01,  1.5923e-01,  2.2849e-01, -3.5337e-02,\n",
      "         -4.4611e-01,  1.7631e-01, -2.1702e-01, -3.3798e-01, -3.0157e-01,\n",
      "          5.5035e-01, -9.9519e-02, -3.4110e-01,  8.4691e-02,  2.2216e-01,\n",
      "          3.6868e-01,  4.2618e-01,  3.5346e-01, -7.4891e-02, -2.0437e-01,\n",
      "         -2.0130e-01,  2.3036e-01, -4.9480e-01, -2.1039e-01,  1.0243e-01,\n",
      "          2.8005e-01,  4.5913e-02, -2.8801e-01,  2.2953e-01,  2.4560e-01,\n",
      "         -5.0299e-01,  1.4498e-01,  4.1948e-01, -8.6418e-02, -8.5082e-02,\n",
      "          2.3914e-01,  1.5430e-01,  1.1531e-01,  2.0084e-01, -1.9783e-02,\n",
      "         -1.9817e-02, -2.0000e-01, -2.2991e-01,  1.3297e-01, -1.6627e-01,\n",
      "          2.0833e-01, -1.1981e-02, -9.9746e-02,  6.6959e-02,  4.7438e-02,\n",
      "         -3.1745e-01,  2.0113e-02,  1.5151e-01,  1.7016e-01, -1.7242e-01,\n",
      "          2.4331e-01,  9.7360e-02, -4.9702e-02, -2.8961e-01, -2.7300e-01,\n",
      "          1.7337e-01, -1.8410e-01,  3.7468e-01, -3.0401e-01,  8.1021e-01,\n",
      "          6.2542e-02,  2.0152e-01,  2.0510e-01, -9.6186e-02,  2.2079e-02,\n",
      "          4.6016e-01, -1.5199e-01, -1.3173e-01, -1.7357e-02,  3.5436e-01,\n",
      "          2.0621e-01,  1.8573e-01, -2.2052e-01, -1.9822e-01, -1.1156e-01,\n",
      "         -4.9773e-01,  5.8261e-01, -1.7482e-01,  3.8446e-02, -3.8843e-01,\n",
      "         -4.3666e-01, -3.4758e-01, -2.4286e-01,  2.9600e-01, -8.2504e-01,\n",
      "          2.2585e-01,  3.8215e-01,  9.4056e-02, -5.1324e-01, -3.3844e-01,\n",
      "          1.5736e-01, -3.4283e-01,  4.9392e-03, -4.4920e-02,  2.1017e-01,\n",
      "         -1.9187e-02, -5.5771e-01, -3.2355e-01,  3.0427e-01,  2.3797e-01,\n",
      "         -3.7322e-01, -8.8939e-02,  3.6101e-01, -2.6191e-01,  1.3055e-01,\n",
      "         -2.9792e-02, -3.1843e-01, -1.0924e-01,  2.2391e-01, -4.8320e-01,\n",
      "         -1.7469e-01, -5.6531e-01,  9.4863e-02, -6.8726e-02,  2.5847e-01,\n",
      "          4.0988e-01, -2.0285e-04,  2.2565e-01,  2.0990e-01, -1.1218e-01,\n",
      "         -6.6922e-01,  1.4643e-01,  1.2238e-02, -3.1938e-01, -3.4949e-01,\n",
      "          1.2552e-01,  2.1153e-01, -1.7876e-01, -6.7988e-01,  3.7657e-01,\n",
      "          6.8072e-02, -2.6502e-01, -3.2734e-02,  2.8937e-01, -1.0688e-01,\n",
      "          1.3580e-01,  4.0621e-01, -9.5809e-02, -3.9635e-01, -9.5455e-02,\n",
      "         -2.9494e-01, -1.7154e-03,  9.1559e-05, -1.6935e-01, -4.0224e-02,\n",
      "         -2.9197e-01, -8.7357e-01, -3.6428e-01,  1.0001e-01,  6.1706e-02,\n",
      "          2.8385e-01,  3.4620e-01,  9.7510e-02,  1.2381e-01,  2.3785e-01,\n",
      "         -7.9275e-02,  3.1086e-01, -3.4406e-01,  4.6712e-01, -1.3325e-02,\n",
      "         -7.6744e-01,  2.0022e-01, -3.0514e-02,  2.0091e-01, -3.4264e-02,\n",
      "          3.0825e-01,  4.4061e-01,  2.4213e-01, -3.2104e-01,  3.9499e-02,\n",
      "          1.3149e-03,  3.4643e-01,  9.1886e-02, -3.3358e-02,  8.2561e-02,\n",
      "          3.8938e-01,  1.0233e-01,  2.5602e-02,  4.4739e-01,  2.3142e-01,\n",
      "         -1.8545e-01, -1.6326e-01, -2.1837e-01, -3.3851e-01,  1.0990e-01,\n",
      "          2.6477e-01, -1.2402e-01, -7.5830e-02, -4.8455e-01, -2.4958e-01,\n",
      "          2.2054e-01,  1.4217e-01,  6.3064e-02,  2.5731e-01,  1.2959e-01,\n",
      "         -2.8986e-01, -3.0717e-01,  5.2182e-01, -1.1852e-01,  3.1585e-01,\n",
      "          4.3921e-01,  7.9643e-02,  6.0691e-01, -8.1346e-01,  2.6087e-01,\n",
      "          4.3017e-01, -3.9511e-01,  3.6981e-01, -3.3264e-02,  6.9453e-02,\n",
      "         -1.9201e-01,  3.1550e-01, -1.3094e-02,  1.1804e-01,  1.3435e-01,\n",
      "          3.7127e-01, -2.8720e-01,  2.9595e-01,  1.5554e-01,  3.5119e-01,\n",
      "          2.7650e-01, -3.9605e-01, -3.5674e-01, -1.6986e-01,  2.9248e-01,\n",
      "         -1.4123e-01, -7.6491e-01, -2.6632e-02,  1.8660e-01, -4.0915e-02,\n",
      "          1.2524e-01,  3.7448e-02,  1.4786e-01, -1.2974e-02, -1.6354e-02,\n",
      "          1.6076e-01, -1.6572e-01,  7.6164e-02,  9.1253e-02,  1.5065e-01,\n",
      "          2.1397e-01,  5.3322e-02, -1.2698e-01,  7.5630e-01, -1.4472e-01,\n",
      "         -1.8620e-01,  4.6358e-01, -9.9080e-02, -5.4927e-01,  2.8709e-01,\n",
      "         -3.1658e-01, -1.6150e-01,  3.9651e-01,  1.5916e-01, -3.1279e-01,\n",
      "          1.5678e-01,  5.8094e-01,  1.7289e-01, -2.6301e-03, -3.5711e-01,\n",
      "          2.2629e-01,  8.8209e-02,  3.0865e-01,  3.9493e-01,  2.0537e-01,\n",
      "          2.4234e-01,  5.3979e-02, -2.5349e-01, -2.4375e-01,  1.1660e-01,\n",
      "          2.1952e-02, -4.3271e-01,  9.5115e-02, -6.7516e-01,  1.2579e-01,\n",
      "         -1.0118e-01,  1.4677e-01,  5.9849e-01,  1.9837e-01, -4.0696e-01,\n",
      "         -3.5222e-01, -5.7574e-02, -1.2954e-01,  5.6281e-02, -5.9488e-02,\n",
      "         -1.7732e-03,  2.6385e-02,  3.0747e-02, -1.5234e-01,  2.3961e-02,\n",
      "          4.3650e-01, -4.5213e-01,  5.0585e-02, -2.0746e-01,  6.6741e-02,\n",
      "         -2.2484e-01,  2.9138e-01,  3.4101e-01, -1.7138e-01, -1.5247e-02,\n",
      "          2.9813e-01, -2.5210e-01,  2.1369e-01,  4.7890e-02,  2.3677e-02,\n",
      "         -7.5913e-02, -2.5187e-01, -3.1273e-01, -2.5875e-01, -6.1864e-01,\n",
      "          6.5894e-02,  2.7351e-02, -5.5953e-02,  1.5455e-01, -4.6169e-02,\n",
      "         -2.2973e-01, -8.9710e-03,  3.6405e-01, -1.0458e-01,  2.3538e-01,\n",
      "         -3.5612e-01,  3.2054e-01, -1.0029e-01,  1.9959e-01, -6.3612e-01,\n",
      "          3.6196e-01, -3.3022e-02, -1.9228e-02, -1.3959e-01,  3.5104e-01,\n",
      "          6.7756e-02,  6.1758e-01, -8.0924e-02, -3.4956e-01,  8.6694e-02,\n",
      "          1.8112e-01, -8.7314e-02, -5.6920e-02,  2.1571e-01, -2.2475e-02,\n",
      "          3.1272e-01,  1.2476e-01, -1.5275e-01, -4.6872e-01, -2.0702e-01,\n",
      "         -1.7028e-01, -6.6093e-01,  3.0224e-02,  5.0307e-01, -1.2667e-01,\n",
      "          9.2418e-02,  7.8994e-04, -2.1163e-01, -9.6065e-02, -2.3173e-01,\n",
      "          8.3925e-02, -4.1911e-01,  5.1711e-02,  1.1025e-01,  4.5248e-01,\n",
      "         -2.2567e-01, -1.1668e-01, -8.6390e-02,  1.5611e-01, -1.6841e-01,\n",
      "          1.2604e-01,  7.8049e-02,  2.3848e-01,  1.3811e-01, -9.0478e-02,\n",
      "         -1.9003e-02, -3.2634e-01,  6.7299e-01, -1.9088e-01,  1.1114e-01,\n",
      "         -4.5070e-01, -5.9086e-01, -1.3593e-01, -1.8597e-02, -2.5292e-02,\n",
      "         -2.5056e-01,  1.0083e-01, -1.7612e-01,  2.3631e-01,  2.0109e-01,\n",
      "          1.6586e-01, -9.1894e-02,  1.1097e-01, -1.7129e-01,  3.3897e-01,\n",
      "         -1.0025e-01, -2.1402e-01, -1.9622e-01,  2.3227e-01,  2.1618e-01,\n",
      "          2.0915e-02, -3.4867e-01,  1.5043e-01, -3.3880e-01, -1.6082e-01,\n",
      "         -7.7891e-02,  4.7028e-01, -3.6332e-01,  1.5737e-01,  2.4384e-01,\n",
      "         -2.0437e-01, -4.6387e-02, -5.4972e-02, -1.0702e-01, -2.1551e-01,\n",
      "         -5.2605e-02, -7.9134e-02, -2.2733e-01,  3.5867e-01,  1.6564e-01,\n",
      "         -2.5725e-01, -2.7419e-01,  3.8169e-01,  4.6734e-01, -3.7946e-01,\n",
      "          2.1770e-01, -1.2936e-02, -1.8244e-02, -3.7457e-01, -3.1764e-01,\n",
      "          5.4191e-02, -2.8263e-01, -2.7216e-01,  2.9648e-01,  2.1176e-01,\n",
      "          4.1370e-01,  9.8434e-02,  3.3002e-02, -8.0853e-03, -1.8699e-01,\n",
      "         -2.6445e-01,  7.2948e-02, -5.4882e-02, -2.1748e-03,  4.5967e-02,\n",
      "         -1.4736e-01,  1.8632e-01, -1.6193e-01,  2.6586e-01,  4.1174e-01,\n",
      "          1.6233e-02, -1.1236e-01,  2.9398e-02,  2.7988e-01, -5.1480e-01,\n",
      "         -8.7705e-02, -1.7076e-01, -4.3031e-02, -4.4093e-01, -5.1395e-01,\n",
      "         -1.8341e-01, -1.0365e-01, -3.2069e-01,  2.8115e-01,  2.3300e-01,\n",
      "         -2.8313e-01,  5.6549e-03, -1.9071e-01,  1.0052e-01, -2.5348e-02,\n",
      "          2.0382e-01, -1.7545e-01, -8.7508e-02, -8.3541e-02,  8.0762e-02,\n",
      "         -4.4323e-02, -3.9875e-01,  3.7147e-01, -1.2502e-01,  1.4683e-01,\n",
      "          1.6672e-01,  4.6842e-01,  1.4289e-01,  5.5269e-02, -4.3644e-02,\n",
      "          1.4523e-01, -1.1483e-01,  2.6063e-01, -3.8210e-01,  2.1135e-01,\n",
      "          2.7871e-01, -3.6289e-01, -5.3715e-01,  1.5744e-01,  2.2283e-01,\n",
      "         -2.0725e-01,  3.0138e-01,  1.2408e-01,  2.2737e-01,  4.3323e-01,\n",
      "          9.5425e-02, -9.6709e-02, -2.8688e-01, -1.5702e-02,  4.9664e-02,\n",
      "          1.8882e-01, -1.5718e-01,  1.6695e-01,  2.0720e-01,  3.5245e-02,\n",
      "         -2.0648e-01, -4.2842e-02,  3.2667e-01, -2.0083e-01,  7.4800e-01,\n",
      "          4.1494e-02, -1.6950e-01,  1.4546e-01, -3.0381e-01, -7.0179e-02,\n",
      "         -3.2989e-02, -3.1685e-01, -8.3507e-02,  2.3947e-02,  7.3385e-02,\n",
      "          4.2950e-02, -3.2016e-01, -3.1622e-02,  1.3551e-01, -1.3858e-02,\n",
      "          3.3733e-01,  1.0866e-01, -9.3647e-02,  8.0035e-03,  1.1432e-01,\n",
      "         -7.2373e-02, -1.8345e-01, -2.0124e-01, -6.1308e-02,  9.5932e-02,\n",
      "          2.7205e-01, -1.7146e-01, -1.6892e-01,  5.1325e-02, -8.2922e-03,\n",
      "          7.7311e-03, -2.5236e-02,  3.3947e-01, -4.3998e-01, -1.8322e-01,\n",
      "          2.8352e-01, -3.0982e-02, -4.8687e-02,  5.5602e-01, -7.7512e-02,\n",
      "          5.4006e-02, -9.8187e-01,  1.1871e-01,  3.0463e-01, -4.3014e-01,\n",
      "          2.0555e-01,  8.1006e-02,  2.2228e-02,  1.0535e-01,  3.4721e-01,\n",
      "          1.6824e-01, -4.1181e-02, -7.7543e-03,  1.0000e-01,  2.5713e-02,\n",
      "         -2.8266e-01,  3.0915e-02, -4.0216e-01, -2.3342e-01,  1.3222e-01,\n",
      "          6.5709e-02, -4.1900e-01, -3.2249e-01, -8.9545e-02,  1.0459e-01,\n",
      "         -1.7184e-01, -2.0391e-01, -9.3681e-02, -2.5814e-01, -1.2978e-02,\n",
      "          5.4775e-02, -5.9046e-01, -2.0468e-03,  2.5238e-01,  1.6385e-01,\n",
      "         -2.5774e-01,  5.5407e-02,  4.4270e-01, -8.0615e-02,  6.5907e-02,\n",
      "         -1.9699e-01, -2.5023e-01, -3.7112e-01, -9.7775e-02,  7.5245e-02,\n",
      "         -1.2937e-01,  3.9432e-01,  6.0911e-02,  1.2915e-01, -4.5981e-01,\n",
      "         -3.6566e-03, -3.2218e-01, -1.9735e-01,  4.7545e-02,  1.5592e-01,\n",
      "          4.7412e-01, -2.8757e-01, -4.1457e-01, -2.7783e-02,  5.0072e-02,\n",
      "         -6.8726e-01, -2.2730e-01,  2.1570e-01, -1.6110e-01,  6.6305e-01,\n",
      "         -3.7048e-01, -4.3301e-01, -4.1843e-01]])\n"
     ]
    }
   ],
   "source": [
    "def gerar_embedding(texto: str) -> torch.Tensor:\n",
    "    # Carregar o tokenizer e o modelo pré-treinado\n",
    "    tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "    model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "    \n",
    "    # Tokenizar o texto de entrada\n",
    "    inputs = tokenizer(texto, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Obter as saídas do modelo (embeddings)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extraímos o embedding da última camada do BERT\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Opcionalmente, podemos calcular a média dos embeddings das palavras para obter um vetor fixo\n",
    "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86844397\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "embedding1 = gerar_embedding(\"Cachorro\")\n",
    "embedding2 = gerar_embedding(\"Gato\")\n",
    "print(cosine_similarity(embedding1, embedding2)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
