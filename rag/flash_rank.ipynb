{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteca FlashRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata é opcional, Id pode ser seu identificador do banco de dados ou simplesmente índices numéricos.\n",
    "query = \"Quais são os principais desafios e aplicações da IA generativa na indústria atual?\"\n",
    "passages = [\n",
    "   {\n",
    "      \"id\": 1,\n",
    "      \"text\": \"Explicação sobre o que é IA generativa, destacando modelos como GPT e Diffusion, e sua capacidade de criar conteúdo original.\",\n",
    "      \"meta\": {\"importância\": \"alta\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 2,\n",
    "      \"text\": \"Exemplos de aplicações como geração de texto (chatbots), criação de imagens, design de produtos e conteúdo audiovisual (dublagem e roteiros).\",\n",
    "      \"meta\": {\"importância\": \"alta\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 3,\n",
    "      \"text\": \"Desafios éticos, incluindo viés, deepfakes, desinformação e impacto no emprego.\",\n",
    "      \"meta\": {\"importância\": \"alta\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 4,\n",
    "      \"text\": \"Impacto econômico, abordando o potencial para reduzir custos e acelerar processos criativos, bem como os altos investimentos necessários para treinar e implementar modelos.\",\n",
    "      \"meta\": {\"importância\": \"alta\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 5,\n",
    "      \"text\": \"Uso da IA generativa para experiências altamente personalizadas, como recomendações em plataformas de streaming e marketing direcionado.\",\n",
    "      \"meta\": {\"importância\": \"média\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 6,\n",
    "      \"text\": \"Desafios técnicos, como o alinhamento do modelo, controle sobre as saídas e a necessidade de grandes volumes de dados de alta qualidade.\",\n",
    "      \"meta\": {\"importância\": \"média\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 7,\n",
    "      \"text\": \"Adoção na área da saúde, com usos em diagnósticos, simulação de medicamentos e assistência médica personalizada.\",\n",
    "      \"meta\": {\"importância\": \"média\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 8,\n",
    "      \"text\": \"Privacidade e regulamentação, com discussões sobre proteção de dados e legislações emergentes relacionadas à IA.\",\n",
    "      \"meta\": {\"importância\": \"média\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 9,\n",
    "      \"text\": \"Avanços em modelos multimodais que integram texto, imagem e som para criar experiências mais interativas.\",\n",
    "      \"meta\": {\"importância\": \"baixa\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\": 10,\n",
    "      \"text\": \"Cenários futuros da IA generativa, incluindo integração com tecnologias emergentes como realidade aumentada e computação quântica.\",\n",
    "      \"meta\": {\"importância\": \"baixa\"}\n",
    "   }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker(max_length=128)\n",
    "rerankrequest = RerankRequest(query=query, passages=passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.017240638},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.011028998},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.0006223073},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.0002462871},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 8.866134e-05}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rerank(rerankrequest)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MiniLM-L-12-v2...\n",
      "ms-marco-MiniLM-L-12-v2.zip: 100%|██████████| 21.6M/21.6M [00:00<00:00, 35.9MiB/s]\n"
     ]
    }
   ],
   "source": [
    "ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "rerankrequest = RerankRequest(query=query, passages=passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.9489663},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 0.8483421},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.7208167},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.5495378},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.004810058}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rerank(rerankrequest)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading rank-T5-flan...\n",
      "rank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:02<00:00, 36.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "ranker = Ranker(model_name=\"rank-T5-flan\")\n",
    "rerankrequest = RerankRequest(query=query, passages=passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.5551983},\n",
       " {'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.53342193},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 0.508104},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.49806476},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.4208185}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rerank(rerankrequest)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:02<00:00, 36.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\")\n",
    "rerankrequest = RerankRequest(query=query, passages=passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 0.9981872},\n",
       " {'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.99803215},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.9921639},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.9846849},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.22521202}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rerank(rerankrequest)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de rerank entre diferente modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_mapping = {\"alta\": 3, \"média\": 2, \"baixa\": 1}\n",
    "true_ranking = [importance_mapping[passage[\"meta\"][\"importância\"]] for passage in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a serem usados\n",
    "rankers = [\n",
    "    {\"name\": \"Ranker(max_length=128)\", \"instance\": Ranker(max_length=128)},\n",
    "    {\"name\": \"Ranker(model_name='ms-marco-MiniLM-L-12-v2')\", \"instance\": Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")},\n",
    "    {\"name\": \"Ranker(model_name='rank-T5-flan')\", \"instance\": Ranker(model_name=\"rank-T5-flan\")},\n",
    "    {\"name\": \"Ranker(model_name='ms-marco-MultiBERT-L-12')\", \"instance\": Ranker(model_name=\"ms-marco-MultiBERT-L-12\")},\n",
    "]\n",
    "\n",
    "# Função para calcular as métricas\n",
    "def calculate_metrics(true_ranking, predicted_ranking):\n",
    "    # Kendall's Tau\n",
    "    tau, _ = kendalltau(true_ranking, predicted_ranking)\n",
    "    \n",
    "    return tau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG score: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "# Vetor de relevância para uma consulta\n",
    "y_true = np.array([3, 2, 3, 0, 1])  # Relevância dos itens para a consulta\n",
    "\n",
    "# Vetor de pontuações (probabilidades) atribuídas pelo modelo para os itens\n",
    "y_score = np.array([0.8, 0.6, 0.7, 0.1, 0.2])  # Pontuação de cada item\n",
    "\n",
    "# Calcular o NDCG\n",
    "score = ndcg_score([y_true], [y_score])  # Passando como lista de listas\n",
    "\n",
    "print(f\"NDCG score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Ranker(max_length=128)\n",
      "  Kendall's Tau: -0.2635231383473649\n",
      "Modelo: Ranker(model_name='ms-marco-MiniLM-L-12-v2')\n",
      "  Kendall's Tau: -0.3689323936863109\n",
      "Modelo: Ranker(model_name='rank-T5-flan')\n",
      "  Kendall's Tau: 0.05270462766947298\n",
      "Modelo: Ranker(model_name='ms-marco-MultiBERT-L-12')\n",
      "  Kendall's Tau: -0.10540925533894596\n"
     ]
    }
   ],
   "source": [
    "# Processamento para cada modelo\n",
    "results = {}\n",
    "for ranker in rankers:\n",
    "    # Realiza o reranking com o modelo\n",
    "    rerank_request = RerankRequest(query=query, passages=passages)\n",
    "    output = ranker[\"instance\"].rerank(rerank_request)\n",
    "    \n",
    "    # Extrai o ranking gerado pelo modelo (baseado nas pontuações)\n",
    "    predicted_ranking = [result[\"id\"] for result in sorted(output, key=lambda x: x[\"score\"], reverse=True)]\n",
    "    \n",
    "    # Calcula as métricas\n",
    "    tau = calculate_metrics(true_ranking, predicted_ranking)\n",
    "    \n",
    "    # Armazena os resultados\n",
    "    results[ranker[\"name\"]] = {\"Kendall's Tau\": tau}\n",
    "\n",
    "# Exibe os resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"Modelo: {model}\")\n",
    "    print(f\"\"\"  Kendall's Tau: {metrics[\"Kendall's Tau\"]}\"\"\")\n",
    "    #print(f\"  NDCG: {metrics['NDCG']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
